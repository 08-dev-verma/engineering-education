### INTRODUCTION
Technology is fast growing in the world today. This recency in growth is enhancing the livelihoods of humans and their environment. This includes the building of many self-driven cars and some smart digital systems.
A few problems in the understanding of its working algorithm have arisen. 
   
   These problems have triggered developers to ask questions like: "

* What is the design algorithm enhancing deep learning?
* Why are recent developed systems able to recognize humans and their environment?
* How can devices move accurately without human beings controlling them?

      An excessive volume of information (data sets) can be trained. This uses learning algorithms in deep learning. Deep learning systems tend to gather functional facts from larger datasets. This experience (learning) is in relation to human ways of getting smarter. 

#### Do you think deep learning will make current technologies better?

Deep learning is, without a doubt, at the pinnacle of the most current technological hierarchy. The ability to collect data is crucial for systems (information). The data acquired enables exact forecasting and establishes the DL. As a prediction analysis, comprehending the facial recognition algorithm. This analysis functions by collecting a group of consumer behavior data sets. The intelligence and accuracy rate of deep learning are immense, and they are being checked at all levels. Humans and their environments have been great benefactors of this recent smart technology.
 The ability to fine-tune the budget and the considered problem are two enhancing variables in Deep Learning. Any optimizer’s accuracy is measured using these factors. 

### Some Determinants That Transform Deep Learning Systems' Performance

#### i. Taking into account the issue (considered problem) that the system will be tasked with resolving; 
          Simply said, this refers to the task for which the system will be created. This problem, which the system must solve, is the most crucial consideration when modeling an algorithm.
 ######  Basic problems
* What kind of system should be built? 
* How will the developed algorithm handle the immediate user’s demands?
* How easily will re-engineering be made possible?

#### ii. Evaluation difficulties of default hyperparameters for multiple optimisers.
          Poor accuracy is obtained here as a result of low awareness from training the system data sets. The Adams optimiser is used in this deep system algorithm, which is adaptable. 

#### iii. Technical expertise in deep learning system network design.

          The demand and need for deep learning experts is highly recommended here. Experience is needed to increase the learning rate and improve the system's performance.

#### iv. Which optimiser suits the deep network best?
          According to research, the Adam optimiser offers the highest and best performance rate. The ability to outperform other approaches (optimisers) is lacking. 

### How To Get A Perfect Performance Rate From A Deep Learning System

#### 1. A Deep Neural Network System's Individual Units' (Layers') Functionality 

Convolutional neural networks must take individual activation levels into account. This step, though, is difficult to achieve. First, the issue of unit interpretability in deep networks must be grasped.
This knowledge will assist the network in avoiding becoming a completely opaque black box. Our measurement method is network dissection. This method calculates a system's deep learning performance rate.
Measurements of alignments between different levels are also part of network dissection in the deep convolutional network (units). It is being done to separate dense data sets for a quicker performance rate. This segmentation notion is also known as "broden." Other classifier networks can use the Broden approach as well. The Progressive generative adversarial network with the VGG-16 scene classifier (P-GAN). The most common are these two.
The network's training methods are the generator and discriminator progression layers. A convolutional neural network (CNN)-based classifier is made up of two layers. These layers, which are referred to as the earlier and later layers,  The final layer is made up of two levels: objects and parts. The system associates with the layers and emerges when it notices this sub-layer. There is a lot of color dictation in the early levels.
 Research has highlighted the importance of performance rate categorization in CNN neurons. Some deep neural networks are trained to create scenes, while others are trained to recognize them. This helps to improve the data flow in the system. 
Manipulation of GAN's unit-producing outlets is complicated by artificial pruning. Trained images, like adversarial examples, are developed, categorized, and highlighted on certain neurons. These neurons are treated in some way in order to attain precision. 

#### 2. Soft Convolutional Inductive Biases' Effectiveness in Vision Transformation Improvement

The convolution network strongly supports inductive bias layers. The convolution network layers are made up of the concepts of weight sharing and localisation. These layers are based on imaging principles (translation, symmetric transformation, reflection). In visible corrected, activation patterns may be seen. 
On CNNs, training large data sets is difficult. The performance rate of Vision transformers is more consistent. Big data set algorithm systems have demonstrated this stability. When pre-trained, it is also termed a self-attention mechanism. (2021). The concept of Gated Positional Self-Attention (GPSA) has been proposed. This was done to aid in the investigation of the data's quality and performance. To avoid layer locations, GPSA employs self-convolutional inductive bias with freedom. Systems can move through various layers (locations), learn, govern, and collect content-specific data. During their training period, the hyperparameters provide this information. The method dramatically improves the display and correctness of hyperparameters. When compared to the traditional vision transformer method, this improvement is significant. The author goes on to discuss how to use image net samples and parameter efficiency in more detail. 
In its earliest layers, GPSA appears to initialize more hyperparameters. The final (later) levels are more concerned with content data (other distinctive data sets).
 
#### 3. Enough degrees of freedom 

The main challenge here is fine-tuning the deep network's hyperparameters. This is a difficult task because the output's accuracy is in question. 
What key elements are used to determine a network's degree of freedom? 
Basic statistical speculation postulates exist to help examine the likelihood rate of an event. Larsen et al. (2021) suggest a theory to aid in the resolution of the fine-tuning issue. The probability of success in a deep network underpins this idea. This is what is expected to happen when the hyperparameters of a system data set are being trained.
In a deep network, the degree of freedom determines whether or not success (accuracy) is possible. During the training process, the theory has aspects to consider. Gordon's Escape Theorem is generalized by the theory that establishes a powerful theorem. This theorem can be applied to any set that is well-known. In a successful chance, the principal outcome emphasizes the presence of a section transition. It's best to keep dimensionality as low as possible while initializing parameters. Dimensionality influences the sub-level set of network layers.
The theory (lottery subspaces) seeks to repeat data from prior training experiments. It then establishes the settings for a low-dimensional projection. It compares the parameters to such a trajectory's top-dimensional principal components. The result of this theoretical hypothesis is that both postulates are true. For the same compression, subspace-confined neural networks could even outperform lottery tickets. 

 ### CONCULSION
With the vast and rapid development of technology, deep learning is the key to today’s smart world. This has brought an ease in communication between humans and their daily environment. There are some difficulties in understanding the deep learning system algorithm.
Some real-life postulates are cited to help design the deep network algorithm.

There are some factors that affect the performance of  deep learning systems. These factors tend to reduce the accuracy level of the system: These factors include: 
* Considered a problem.
* Evaluation difficulties of default hyperparameters for multiple optimisers.
* Technical know-how
* Which optimiser suits the learning algorithm best?

There are some steps to aim at in order to achieve a perfect performance rate in a deep learning system. Factors like;
* The functionality of individual layers in the deep network system.
* The efficiency of soft convolutional inductive biases in vision transformation improvement.
* Suitable degrees of freedom to aim at in the deep network while training.


### Diagram Demonstrating The Different Scenarios One Can Fall Into When Configuring The Learning Rate.

 ![different_scenarios_one_needs_to_know_when_configurating_the_learning_rate_in_deep_learning] (engineeringeducation/factors_that_affects_the_performance_of_system_using_deep_learning/display_rate_charts.png) 

     Training a model by initialising very low learning rate and gradually increasing the rate is a good way of evaluating the learning rate of the system. This could either be iterated linearly or exponentially according to Leslie N. Smith.

>Note; Less training time, lesser money spent on GPU cloud compute.  

 ![different_scenarios_one_needs_to_know_when_configurating_the_learning_rate_in_deep_learning] (engineering-education/factors_that_affects_the_performance_of_system_using_deep_learning/iteration.png) 

    The plot below shows that the learning rate (log scale) is inversely proportional to the losses. As the learning rate increases, there comes a breakpoint were the losses stops decreasing and gradually starts increasing on the plot. The learning rate goes from right to left as it increases from left to right on the plotted graph. 

![different_scenarios_one_needs_to_know_when_configurating_the_learning_rate_in_deep_learning] (engineeringeducation/factors_that_affects_the_performance_of_system_using_deep_learning/learning_rate_scale.png) 


