### Introduction

   In our modern world, technology has enhanced the building of some smart self-driven cars, smart digital systems such as “Alexa”, and even smart medical and agricultural equipment’s all around us which are been built using the principle of deep learning. This developed technology has been of great benefits to our everyday society. For that reason, the quest in understanding how this AI based technology (Deep learning) works is paramount for a faster production of similar systems. As such, people have come up with questions like; 
   
  **The principle behind deep learning?**  
     Why do smart systems like Alexa recognize human and objects and how can a car drive itself without human beings sitting in to control it?

       Deep Learning is where algorithms learn independently from excessive amounts of information. The system algorithm tends to be smarter having gathered functional experience from more data sets just like humans get smarter by learning.
     
### Why Is Deep Learning Important?
    Deep learning is the trending technology all around us. The ability of a system to cluster(collect) data and make predictions with incredible accuracy makes deep learning. The trained algorithms will recognize individual faces when collecting their photos through snapping, make a predictive analysis of consumer behavior, or detect fraud. Deep learning enables higher percentage of turn in data sets and also increase intelligence in technology. With the rapid growth in technology around us, Deep Learning has uniquely aid in the major breakthrough of technology to almost if not all aspect of life such as medicine, science, agriculture, etc. Deep learning has always greatly enhance the improvement of humans lives when considering technologies like IoT, embedded system and Robotics. Unsurprisingly, considered problem and tuning budget are the two major factors used to checkmate the performance rate of any Deep Learning optimizer.
     
 ### Factors That Affects The Performance Of System Using Deep Learning 

i. **Considered problem;** 
      
      This simply means what task is the system about to be design handling. While modelling an algorithm, this problem is to be considered as most important. 
    
     Basic problems;

     - Why is this system develop?
     - How will the developed system handle the user’s demands,  
     - Re-engineering adaptive ability 

  
ii.  **Evaluation of default hyperparameters for multiple optimisers is not easily achieved**

        This is due the lack of awareness of what is been learnt by the system algorithm using adaptive methods like the Adams optimiser.

iii. **Technical know-how**

       It needs one with a great and deep understanding of deep learning to turn the learning rate schedule to improve the performance system. 

iv.  **Which optimiser suits best**

     Research has it that the optimiser with the best and strongest performance rate is the Adam optimiser. Other methods (optimisers) lacks its outperforming abilities. 

### Steps Involved In Getting A Perfect Performance Rate Of A System

**Individual Units(layers) functionality in a Deep Neural Network must be highly considered**

 Understanding the functional role of individual activation levels in Convolutional Neural Networks is a bit problem. The problem of unit interpretability in deep networks needs to be understood to help hint the deep network and not completely opaque black boxes. Network Dissection is our method for measuring the performance rate of a system in deep learning. The performance rate is interpreted to the network system for each individual network layer in the deep convolutional network. It involves the measurements of alignments between different layers (units) and the segmentation of dense data sets for a faster performance rate are drawn from this concept which is also called Broden. The technique is applied to a VGG-16 scene classifier and Progressive generative adversarial network(P-GAN) trained on kitchen images. Convolutional neural network (CNN) based classifier has two layers namely the earlier and the later layers. The later associates objects and parts emerged in the layers when been observed by the system while colors dictation occurs greatly in the earlier layers. Research shows that classification of accuracy and strong performance of a Deep Learning network is highly important in such a network neurons as CNN. For the generator network based classifier, the reserve case of the Convolution neural network occurs were colors dictation happens at the later layers while objects and part association happens at the earlier layers most frequently. Some deep network systems are trained to generator scenes while some are trained to discriminate scenes. This helps in the enhancement of information flow in the system. Manipulation of unit generating outlets of GAN is considered a challenge in artificial pruning. Designing structured imprinting images and highlighting specific neurons like Adversarial examples are been highly regarded by interested applications due to the reasons of manipulations.

**The efficiency of Soft Convolutional Inductive Biases in vision transformation improvement**

  Inductive bias layers is highly supported by convolution network. The concept of weight sharing and locally notified in the convolution network layers is said to be built on the principles of imagery(translation, symmetric transformation, reflection). Activation patters are observed in visible corrected. Spatial long range data dependencies are not easy to achieve on CNNs despite their strong performance in the small data regime. Vision transformers performance rate tends to be more stable with big data algorithm system as well as it self-attention mechanism when been per-trained. D'Ascoli et al.(2021) introduce Gated Positional Self-Attention (GPSA) to help study the quality of this big data. GPSA uses self convolutional inductive bias with freedom to escape layers localities. This system can navigate through different localities(layers), learns, regulates and collects content information from the hyperparameters during its training period. Hyperparameters visualisation and correctness is greatly improving in the method compare to the standard vision transformer method as the author last further explain using Image net sample and parameter efficiency. Interestingly, GPSA initialises more hyperparameters in its earlier layers while the later focuses more on content data( other distinctive data sets). 

**Suitable degrees of freedom**

  Here the major challenge is how to fine-tune the hyperparameters in the deep neural network for a for a more accurate output. What basic factors are used to measure the degree of freedom in a network. Just like basic statistical speculation postulate exists to help analyse the probability rate of an event turning out successful, Larsen et all.(2021) propose a theory. This theoretical hypothesis is based on probability rate of hitting success when the hyperparameters of a system data sets is under training. The probability of success is directly proportional to the degree of freedom in a deep neural network. The theory has factors to watch during training process like dimensionality, sub-space distance between parameters and sub-level set, geometry, etc. The theory proof a powerful theorem, which generalises Gordon’s Escape Theorem to well-known sets. The primary outcomes highlights the existence of a section transition in the success chance. During the initialisation of parameters, it is best to keep dimensioning simple and clear. The possibility of the subspace been proportional to the loss sub-level set is wholly dependent on dimensionality.
The theory (lottery subspaces) aims at reiterating information from previous training tests, sets parameters to a low dimensional projection, compares the parameters to the top dimensional principal components of such trajectory. This theoretical hypothesis lead to a conclusion that subspace-confined neural networks may even outperform lottery tickets for similar compression.

### Diagram demonstrating the different scenarios one can fall into when configuring the learning rate.

 ![different_scenarios_one_needs_to_know_when_configurating_the_learning_rate_in_deep_learning] (engineeringeducation/factors_that_affects_the_performance_of_system_using_deep_learning/display_rate_charts.png) 

     Training a model by initialising very low learning rate and gradually increasing the rate is a good way of evaluating the learning rate of the system. This could either be iterated linearly or exponentially according to Leslie N. Smith.

>Note; Less training time, lesser money spent on GPU cloud compute.  

 ![different_scenarios_one_needs_to_know_when_configurating_the_learning_rate_in_deep_learning] (engineering-education/factors_that_affects_the_performance_of_system_using_deep_learning/iteration.png) 

    The plot below shows that the learning rate (log scale) is inversely proportional to the losses. As the learning rate increases, there comes a breakpoint were the losses stops decreasing and gradually starts increasing on the plot. The learning rate goes from right to left as it increases from left to right on the plotted graph. 

![different_scenarios_one_needs_to_know_when_configurating_the_learning_rate_in_deep_learning] (engineeringeducation/factors_that_affects_the_performance_of_system_using_deep_learning/learning_rate_scale.png) 


